{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project guidelines\n",
    "\n",
    "**Note:** Use these guidelines if and only if you are pursuing a **final project of your own design**. For those taking the final exam instead of the project, see the (separate) [final exam notebook](https://github.com/wilkens-teaching/info3350-s22/blob/main/final_exam/exam.ipynb).\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "These guidelines are intended for **undergraduates enrolled in INFO 3350**. If you are a graduate student enrolled in INFO 6350, you're welcome to consult the information below, but you have wider latitude to design and develop your project in line with your research goals.\n",
    "\n",
    "### The task\n",
    "\n",
    "Your task is to: identify an interesting problem connected to the humanities or humanistic social sciences that's addressable with the help of computational methods, formulate a hypothesis about it, devise an experiment or experiments to test your hypothesis, present the results of your investigations, and discuss your findings.\n",
    "\n",
    "These tasks essentially replicate the process of writing an academic paper. You can think of your project as a paper in miniature.\n",
    "\n",
    "You are free to present each of these tasks as you see fit. You should use narrative text (that is, your own writing in a markdown cell), citations of others' work, numerical results, tables of data, and static and/or interactive visualizations as appropriate. Total length is flexible and depends on the number of people involved in the work, as well as the specific balance you strike between the ambition of your question and the sophistication of your methods. But be aware that numbers never, ever speak for themselves. Quantitative results presented without substantial discussion will not earn high marks. \n",
    "\n",
    "Your project should reflect, at minimum, ten or more hours of work by each participant, though you will be graded on the quality of your work, not the amount of time it took you to produce it.\n",
    "\n",
    "#### Pick an important and interesting problem!\n",
    "\n",
    "No amount of technical sophistication will overcome a fundamentally uninteresting problem at the core of your work. You have seen many pieces of successful computational humanities research over the course of the semester. You might use these as a guide to the kinds of problems that interest scholars in a range of humanities disciplines. You may also want to spend some time in the library, reading recent books and articles in the professional literature. **Problem selection and motivation are integral parts of the project.** Do not neglect them.\n",
    "\n",
    "### Format\n",
    "\n",
    "You should submit your project as a Jupyter notebook, along with all data necessary to reproduce your analysis. If your dataset is too large to share easily, let us know in advance so that we can find a workaround. If you have a reason to prefer a presentation format other than a notebook, likewise let us know so that we can discuss the options.\n",
    "\n",
    "Your report should have four basic sections (provided in cells below for ease of reference):\n",
    "\n",
    "1. **Introduction and hypothesis.** What problem are you working on? Why is it interesting and important? What have other people said about it? What do you expect to find?\n",
    "2. **Corpus, data, and methods.** What data have you used? Where did it come from? How did you collect it? What are its limitations or omissions? What major methods will you use to analyze it? Why are those methods the appropriate ones?\n",
    "3. **Results.** What did you find? How did you find it? How should we read your figures?\n",
    "4. **Discussion and conclusions.** What does it all mean? Do your results support your hypothesis? Why or why not? What are the limitations of your study and how might those limitations be addressed in future work?\n",
    "\n",
    "Within each of those sections, you may use as many code and markdown cells as you like. You may, of course, address additional questions or issues not listed above.\n",
    "\n",
    "All code used in the project should be present in the notebook (except for widely-available libraries that you import), but **be sure that we can read and understand your report in full without rerunning the code**. Be sure, too, to explain what you're doing along the way, both by describing your data and methods and by writing clean, well commented code.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This project takes the place of the take-home final exam for the course. It is worth 20% of your overall grade. You will be graded on the quality and ambition of each aspect of the project. No single component is more important than the others.\n",
    "\n",
    "### Practical details\n",
    "\n",
    "* The project is due at **11:59pm EST on Thursday, May 19, 2022** via upload to CMS of a single zip file containing your fully executed Jupyter notebook and all associated data.\n",
    "* You may work alone or in a group of up to three total members.\n",
    "    * If you work in a group, be sure to list the names of the group members.\n",
    "    * For groups, create your group on CMS and submit one notebook for the entire group. **Each group member should also submit an individual statement of responsibility** that describes in general terms who performed which parts of the project.\n",
    "* You may post questions on Ed, but should do so privately (visible to course staff only).\n",
    "* Interactive visualizations do not always work when embedded in shared notebooks. If you plan to use interactives, you may need to host them elsewhere and link to them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cornell subreddit is a notoriously busy subreddit. With more than 40k users it safe to say that a large part of the cornell popullation is currently or has at some point come across the r/Cornell sub. Many people go to the cornell subreddit looking for adivce, friends. iclickers, or even just companionship. However reddit as a whole has been known to be a website filled with users whose post are more of a creative writing exercise rather than an accruate representation of their beliefs. For this reason I am interesting in seeing if a machine learning model can detect satire of sh*t-posting as it is refered to on the reditt. Satirical texual documents are notoriously difficult to detect even for human readers. Thus it will be interesting to see how a model compares to human labels. I hypothesis that both human and machine labels will not be fully accurate however a human reader might more accurately detect satire than a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code provided in part by github user parth647 to scrape post from a subreddit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (7.6.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Create an instance of reddit class\n",
    "reddit = praw.Reddit(client_id=\"GI_ZqSXXcV12KPsNELGvAw\", #my client id\n",
    "                     client_secret=\"G5uosCAwkAEPRag1h8x01NfYosmy0g\",  #your client secret\n",
    "                     user_agent=\"Pedro_Velazquez\", #user agent name\n",
    "                     username = \"pedrov718\",     # your reddit username\n",
    "                     password = \"Chispit@73\")     # your reddit password\n",
    "\n",
    "\n",
    "# Create sub-reddit instance\n",
    "subreddit_name = \"Cornell\"\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "cornell = pd.DataFrame() # creating dataframe for displaying scraped data\n",
    "\n",
    "# creating lists for storing scraped data\n",
    "titles=[]\n",
    "scores=[]\n",
    "ids=[]\n",
    "body = []\n",
    "\n",
    "# looping over posts and scraping it\n",
    "for submission in subreddit.top(limit=None):\n",
    "    titles.append(submission.title)\n",
    "    scores.append(submission.score) #upvotes\n",
    "    ids.append(submission.id)\n",
    "    body.append(submission.selftext)\n",
    "    \n",
    "    \n",
    "cornell['Title'] = titles\n",
    "cornell['Id'] = ids\n",
    "cornell['Upvotes'] = scores #upvotes\n",
    "cornell[\"body\"] = body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Id</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>if only I knew, nine months ago. if only</td>\n",
       "      <td>hgbtg4</td>\n",
       "      <td>295</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>early morning beebe lake üçÇ</td>\n",
       "      <td>qmm8ig</td>\n",
       "      <td>258</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Grateful</td>\n",
       "      <td>jzb5ol</td>\n",
       "      <td>272</td>\n",
       "      <td>With everything going on in the world, this su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>A car saw me and actually sped up at a ctown i...</td>\n",
       "      <td>qtttqp</td>\n",
       "      <td>239</td>\n",
       "      <td>At least I was fast enough. Some of these fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Cancel culture sucks. Seriously, people need t...</td>\n",
       "      <td>qkn5zn</td>\n",
       "      <td>377</td>\n",
       "      <td>He made a mistake everyone is going overboard....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Haven't seen the Taliban on campus...</td>\n",
       "      <td>ptz8l3</td>\n",
       "      <td>358</td>\n",
       "      <td>Huge shout out to the ROTC, they're really doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Some of y'all really need to revaluate how you...</td>\n",
       "      <td>n2jka5</td>\n",
       "      <td>539</td>\n",
       "      <td>I know many professors have taken advantage of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>How I feel at Cornell sometimes</td>\n",
       "      <td>ix0dvx</td>\n",
       "      <td>391</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>7 am strolls at the cascadilla gorge trail</td>\n",
       "      <td>mqolzc</td>\n",
       "      <td>319</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>There are two types of people in this world</td>\n",
       "      <td>nyihzr</td>\n",
       "      <td>303</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Just wanted to thank everyone for following th...</td>\n",
       "      <td>j3a2v5</td>\n",
       "      <td>287</td>\n",
       "      <td>The majority of us would have sworn on Ezra Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>It is what it is</td>\n",
       "      <td>je88xl</td>\n",
       "      <td>257</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Good Grief! I Miss You All.</td>\n",
       "      <td>g80kan</td>\n",
       "      <td>416</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>'20-'21 Student Assembly Decision Making Process</td>\n",
       "      <td>n0nlf8</td>\n",
       "      <td>275</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Masturbating during the 2940 prelim</td>\n",
       "      <td>ds9js5</td>\n",
       "      <td>253</td>\n",
       "      <td>I don't know what just happened. I was sitting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Sunset state of mind.</td>\n",
       "      <td>leqmxd</td>\n",
       "      <td>296</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>The Decade in Review</td>\n",
       "      <td>ehtjso</td>\n",
       "      <td>329</td>\n",
       "      <td>As the decade comes to a close and we draw 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>I am still at the OS prelim that started yeste...</td>\n",
       "      <td>q90pri</td>\n",
       "      <td>389</td>\n",
       "      <td>edit: here's the [followup](https://www.reddit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>how Cornell random group projects be lookin üò≠</td>\n",
       "      <td>tu2v5e</td>\n",
       "      <td>274</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>‚Äò20</td>\n",
       "      <td>fkk1kt</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title      Id  Upvotes  \\\n",
       "551           if only I knew, nine months ago. if only  hgbtg4      295   \n",
       "786                         early morning beebe lake üçÇ  qmm8ig      258   \n",
       "695                                           Grateful  jzb5ol      272   \n",
       "980  A car saw me and actually sped up at a ctown i...  qtttqp      239   \n",
       "252  Cancel culture sucks. Seriously, people need t...  qkn5zn      377   \n",
       "311              Haven't seen the Taliban on campus...  ptz8l3      358   \n",
       "46   Some of y'all really need to revaluate how you...  n2jka5      539   \n",
       "218                    How I feel at Cornell sometimes  ix0dvx      391   \n",
       "444         7 am strolls at the cascadilla gorge trail  mqolzc      319   \n",
       "513        There are two types of people in this world  nyihzr      303   \n",
       "604  Just wanted to thank everyone for following th...  j3a2v5      287   \n",
       "850                                   It is what it is  je88xl      257   \n",
       "174                        Good Grief! I Miss You All.  g80kan      416   \n",
       "662   '20-'21 Student Assembly Decision Making Process  n0nlf8      275   \n",
       "888                Masturbating during the 2940 prelim  ds9js5      253   \n",
       "557                              Sunset state of mind.  leqmxd      296   \n",
       "433                               The Decade in Review  ehtjso      329   \n",
       "215  I am still at the OS prelim that started yeste...  q90pri      389   \n",
       "699      how Cornell random group projects be lookin üò≠  tu2v5e      274   \n",
       "825                                                ‚Äò20  fkk1kt      256   \n",
       "\n",
       "                                                  body  \n",
       "551                                                     \n",
       "786                                                     \n",
       "695  With everything going on in the world, this su...  \n",
       "980  At least I was fast enough. Some of these fuck...  \n",
       "252  He made a mistake everyone is going overboard....  \n",
       "311  Huge shout out to the ROTC, they're really doi...  \n",
       "46   I know many professors have taken advantage of...  \n",
       "218                                                     \n",
       "444                                                     \n",
       "513                                                     \n",
       "604  The majority of us would have sworn on Ezra Co...  \n",
       "850                                                     \n",
       "174                                                     \n",
       "662                                                     \n",
       "888  I don't know what just happened. I was sitting...  \n",
       "557                                                     \n",
       "433  As the decade comes to a close and we draw 10 ...  \n",
       "215  edit: here's the [followup](https://www.reddit...  \n",
       "699                                                     \n",
       "825                                                     "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cornell.shape)\n",
    "cornell.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I just got a cornell alert saying to avoid the...\n",
       "1      I emailed my chem prof about being very sick a...\n",
       "2                                                       \n",
       "3      I summarized their summary with most of the bi...\n",
       "4                                                       \n",
       "                             ...                        \n",
       "994    If Cornell‚Äôs endowment is SOOOO big, why am I ...\n",
       "995    I don't know what is wrong with you. You leave...\n",
       "996                                                     \n",
       "997                                                     \n",
       "998                                                     \n",
       "Name: body, Length: 999, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re \n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(dataframe, target_column_name, new_column_name):\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x: \" \".join([item for item in x.split() if item not in stopwords.words(\"english\")]))\n",
    "    return(dataframe)\n",
    "def remove_punctuations(dataframe,target_column_name, new_column_name):\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))  \n",
    "    return(dataframe)\n",
    "def stem_text(dataframe, target_column_name, new_column_name):\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x: ps.stem(word) for word in x)\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = remove_stop_words(cornell, \"body\", \"body_clean\")\n",
    "\n",
    "cleaned_data = remove_punctuations(cleaned_data, \"body_clean\", \"no_stops\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# clean_data = stem_text(cleaned_data, \"no_stops\", \"stemmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Id</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>body</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cornell Alert: Anyone know whats going on?</td>\n",
       "      <td>qov789</td>\n",
       "      <td>1113</td>\n",
       "      <td>I just got a cornell alert saying to avoid the...</td>\n",
       "      <td>I got cornell alert saying avoid arts quad, an...</td>\n",
       "      <td>I got cornell alert saying avoid arts quad any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I threw up in my mask and had to continue taki...</td>\n",
       "      <td>qx9bkc</td>\n",
       "      <td>1057</td>\n",
       "      <td>I emailed my chem prof about being very sick a...</td>\n",
       "      <td>I emailed chem prof sick said I either take pr...</td>\n",
       "      <td>I emailed chem prof sick said I either take pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this professor gets it</td>\n",
       "      <td>k3ejjk</td>\n",
       "      <td>1053</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An actual summary of the 97 page report</td>\n",
       "      <td>hdvn9a</td>\n",
       "      <td>901</td>\n",
       "      <td>I summarized their summary with most of the bi...</td>\n",
       "      <td>I summarized summary bits affect students I wo...</td>\n",
       "      <td>I summarized summary bits affect students I wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am a New Bus!</td>\n",
       "      <td>tsqsuw</td>\n",
       "      <td>816</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title      Id  Upvotes  \\\n",
       "0         Cornell Alert: Anyone know whats going on?  qov789     1113   \n",
       "1  I threw up in my mask and had to continue taki...  qx9bkc     1057   \n",
       "2                             this professor gets it  k3ejjk     1053   \n",
       "3            An actual summary of the 97 page report  hdvn9a      901   \n",
       "4                                    I am a New Bus!  tsqsuw      816   \n",
       "\n",
       "                                                body  \\\n",
       "0  I just got a cornell alert saying to avoid the...   \n",
       "1  I emailed my chem prof about being very sick a...   \n",
       "2                                                      \n",
       "3  I summarized their summary with most of the bi...   \n",
       "4                                                      \n",
       "\n",
       "                                          body_clean  \\\n",
       "0  I got cornell alert saying avoid arts quad, an...   \n",
       "1  I emailed chem prof sick said I either take pr...   \n",
       "2                                                      \n",
       "3  I summarized summary bits affect students I wo...   \n",
       "4                                                      \n",
       "\n",
       "                                            no_stops  \n",
       "0  I got cornell alert saying avoid arts quad any...  \n",
       "1  I emailed chem prof sick said I either take pr...  \n",
       "2                                                     \n",
       "3  I summarized summary bits affect students I wo...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving scraped data to my machine \n",
    "cleaned_data.to_csv(\"cornell_reddit_posts.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it is time to train a model to detect satire. At first we will try a simple apporach using NaiveBayes classifier trained on a labeled dataset of news article headlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "2  https://local.theonion.com/mom-starting-to-fea...   \n",
      "3  https://politics.theonion.com/boehner-just-wan...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "\n",
      "                                            headline  is_sarcastic  \n",
      "0  former versace store clerk sues over secret 'b...             0  \n",
      "1  the 'roseanne' revival catches up to our thorn...             0  \n",
      "2  mom starting to fear son's web series closest ...             1  \n",
      "3  boehner just wants wife to listen, not come up...             1  \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "data = pd.read_json(\"Sarcasm.json\", lines=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "2  https://local.theonion.com/mom-starting-to-fea...   \n",
      "3  https://politics.theonion.com/boehner-just-wan...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "\n",
      "                                            headline is_sarcastic  \n",
      "0  former versace store clerk sues over secret 'b...  Not Sarcasm  \n",
      "1  the 'roseanne' revival catches up to our thorn...  Not Sarcasm  \n",
      "2  mom starting to fear son's web series closest ...      Sarcasm  \n",
      "3  boehner just wants wife to listen, not come up...      Sarcasm  \n",
      "4  j.k. rowling wishes snape happy birthday in th...  Not Sarcasm  \n"
     ]
    }
   ],
   "source": [
    "data[\"is_sarcastic\"] = data[\"is_sarcastic\"].map({0: \"Not Sarcasm\", 1: \"Sarcasm\"})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data = data[[\"headline\", \"is_sarcastic\"]]\n",
    "x = np.array(data[\"headline\"])\n",
    "y = np.array(data[\"is_sarcastic\"])\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x) # Fit the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8448146761512542\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to test the accuracy of our Naive Bayes Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = {}\n",
    "for title in cornell.Title:\n",
    "    user = str(title)\n",
    "    data = cv.transform([user]).toarray()\n",
    "    output = model.predict(data)\n",
    "    titles[title] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bodies = {}\n",
    "for post in cornell[\"no_stops\"]:\n",
    "    data = cv.transform([post]).toarray()\n",
    "    output = model.predict(data)\n",
    "    bodies[post] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I got cornell alert saying avoid arts quad anyone idea whats happening Everyone please safe hopefully resolved soon Edit We going get ahead turn megathread situation Edit 2 We put subreddit restricted mode temporarily avoid mountain posts New posts show now still comment Edit 3 If see people giving advice leaving sheltered please report it dont want people get hurt following bad advice Everyone please stay safe locked unless directed otherwise authorities Edit 4 Cornell confirmed bomb threat We still dont know anything else please stay safe locked down Edit 5 We took subreddit restricted mode Still keep stuff related thread post again</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I emailed chem prof sick said I either take prelim as long it‚Äôs Covid would give failing prelim grade So I come take exam person low behold I throw halfway mask run bathroom I come back waiting hallway doesn‚Äôt say anything I ask another mask mine ruined says I take holding paper towel face rest time He asked I one emailed I say yes doesn‚Äôt say anything else I ask I take prelim hallway i‚Äôm spreading germs everyone says yes I finish exam throwing one time time another great look chem departmentü§†</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Not Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I summarized summary bits affect students I would still encourage read report Health Monitoring  Daily check maybe live httpsdailycheckcornelledu COVID symptoms  Possibly require flu vaccine  Face masks policy live httpsehscornelleducampushealthsafetyoccupationalhealthcovid19facecoveringandmaskrequirements  Contact tracing recommended via phone app Testing  Low threshold testing including contact Covidpositive cases  ‚ÄúSurveillance Testing‚Äù regularly testing students staff  Test students travel Ithaca remotely arrive  Testing students already Ithaca movein Movein may 4 8 days accommodate  Reactivation key card NetID requires completing checklist QuarantineIsolationContact Tracing  Quarantine permanent residence Ithaca hotels including possibly Statler  ‚ÄúStudents provided immunity university disciplinary violations activities disclose contact tracing‚Äù Modifications Academic Activities  Later start school year breaks  2 modes instruction online inperson remote access  Classroom capacity reduced 1324  Wear masks classroom sit assigned seats bruh  Virtual OH encouraged  Barton Hall large spaces repurposed quiet study spaces limit gatherings elsewhere  Regular grading reinstated  Attendance shouldn‚Äôt counted credit taken facilitate contact tracing  Recommend stricter cap 18 credit hours mental health  Orientation focus behavioral expectations Modifications student life  Student Organizations encouraged virtual activities among considerations  Greek Life ‚ÄúThe university develop addendum Risk Management Social Events Policy requires compliance NYS local public health guidelines wearing masks visitors house registration public health monitors events coordination SCL ensure vendors healthy safe registering addresses chapter annexes‚Äù  ‚ÄúSome advocated strict enforcement strong sanctions others caution risks pushing activities ‚Äúunderground‚Äù may difficult enforce Greek leaders expressed desire involved enforcement behavioral expectations maintain close partnership university leadership ‚Äú  ‚ÄúThe university suspend inperson concerts lectures involve outside guests promote innovative approaches entirely new ways socializing distancing‚Äù Housing  Eliminate quads triples  Rooms assigned bathrooms reduce  people sharing  Lounges kitchens remain open  Dining halls provide togo service tables properly spaced cutlery disposable reservations required dinein Dedensification  ‚ÄúCampus dedensified inviting none students back residential instruction If option pursued following student groups given priority new students including transfer students residential advisors graduating seniors especially spring athletes if competition take place sport students would otherwise able maintain academic progress without access campus students programs require handson access special facilities lack access internet quiet learning spaces home‚Äù  Comments suggest ideal That say proposing option NOT saying done In fact committee recommends AGAINST this ‚ÄùCampaign Public Health Behavioral Influence Strategies‚Äù  ‚ÄúRecommend system progressive sanctions Initial response would involve student educational Subsequent violations would involve parentlegal guardian student signed FERPA waiver Students could lose access university facilities ultimately referred Office Judicial Administrator repeated violations necessitate formal discipline removal enrollment‚Äù</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 Color red 2 Animal bear 3 Winter cold 4 People stressed</th>\n",
       "      <td>Not Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162 Trick everybody reddit thinking going propose partner Duffield never show up</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woooooo</th>\n",
       "      <td>Not Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Just evict Covid testers gorgeous browsing library put literally anywhere else many rooms less pretty campus put free popcorn back alcove put tables chairs lobby make Willard Straight good again Make campus human place Also make Memorial Room lounge It long until 90s I think would simple add nice leather chairs coffee tables lamps easily moved aside SA meetings formals meetings gatherings WSH valuable place campus whole point building provide social space students Why embrace original mission already have</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If Cornell‚Äôs endowment SOOOO big I wiping single ply üòêüòêüòê</th>\n",
       "      <td>Not Sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I know wrong you You leave trash tables You serve food counter plate You cant use basic app You cant even wear masks noses weve year half Well congratulations Youve played yourselves This years freshmen actually bad student workers quitting dining hall run even worse already does Have fun waiting hour tenth serving orange chicken</th>\n",
       "      <td>Sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              0\n",
       "I got cornell alert saying avoid arts quad anyo...      Sarcasm\n",
       "I emailed chem prof sick said I either take pre...      Sarcasm\n",
       "                                                    Not Sarcasm\n",
       "I summarized summary bits affect students I wou...      Sarcasm\n",
       "1 Color red 2 Animal bear 3 Winter cold 4 Peopl...  Not Sarcasm\n",
       "...                                                         ...\n",
       "162 Trick everybody reddit thinking going propo...      Sarcasm\n",
       "Woooooo                                             Not Sarcasm\n",
       "Just evict Covid testers gorgeous browsing libr...      Sarcasm\n",
       "If Cornell‚Äôs endowment SOOOO big I wiping singl...  Not Sarcasm\n",
       "I know wrong you You leave trash tables You ser...      Sarcasm\n",
       "\n",
       "[326 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(bodies, orient= \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing labelled data to my machine\n",
    "#I will be using this for the rest of my porject\n",
    "\n",
    "labeled = pd.read_csv(\"cornell_reddit_posts_labled.csv\",  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Id</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Label</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "      <th>new_body</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cornell Alert: Anyone know whats going on?</td>\n",
       "      <td>qov789</td>\n",
       "      <td>1111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I just got a cornell alert saying to avoid the...</td>\n",
       "      <td>I just got a cornell alert saying to avoid the...</td>\n",
       "      <td>I just got a cornell alert saying to avoid the...</td>\n",
       "      <td>I got cornell alert saying avoid arts quad, an...</td>\n",
       "      <td>I got cornell alert saying avoid arts quad any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I threw up in my mask and had to continue taki...</td>\n",
       "      <td>qx9bkc</td>\n",
       "      <td>1049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I emailed my chem prof about being very sick a...</td>\n",
       "      <td>I emailed my chem prof about being very sick a...</td>\n",
       "      <td>I emailed my chem prof about being very sick a...</td>\n",
       "      <td>I emailed chem prof sick said I either take pr...</td>\n",
       "      <td>I emailed chem prof sick said I either take pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this professor gets it</td>\n",
       "      <td>k3ejjk</td>\n",
       "      <td>1047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An actual summary of the 97 page report</td>\n",
       "      <td>hdvn9a</td>\n",
       "      <td>908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I summarized their summary with most of the bi...</td>\n",
       "      <td>I summarized their summary with most of the bi...</td>\n",
       "      <td>I summarized their summary with most of the bi...</td>\n",
       "      <td>I summarized summary bits affect students I wo...</td>\n",
       "      <td>I summarized summary bits affect students I wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am a New Bus!</td>\n",
       "      <td>tsqsuw</td>\n",
       "      <td>811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title      Id  Upvotes  Label  \\\n",
       "0         Cornell Alert: Anyone know whats going on?  qov789     1111    0.0   \n",
       "1  I threw up in my mask and had to continue taki...  qx9bkc     1049    0.0   \n",
       "2                             this professor gets it  k3ejjk     1047    0.0   \n",
       "3            An actual summary of the 97 page report  hdvn9a      908    0.0   \n",
       "4                                    I am a New Bus!  tsqsuw      811    0.0   \n",
       "\n",
       "                                                body  \\\n",
       "0  I just got a cornell alert saying to avoid the...   \n",
       "1  I emailed my chem prof about being very sick a...   \n",
       "2                                                NaN   \n",
       "3  I summarized their summary with most of the bi...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  I just got a cornell alert saying to avoid the...   \n",
       "1  I emailed my chem prof about being very sick a...   \n",
       "2                                                NaN   \n",
       "3  I summarized their summary with most of the bi...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            new_body  \\\n",
       "0  I just got a cornell alert saying to avoid the...   \n",
       "1  I emailed my chem prof about being very sick a...   \n",
       "2                                                NaN   \n",
       "3  I summarized their summary with most of the bi...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                          body_clean  \\\n",
       "0  I got cornell alert saying avoid arts quad, an...   \n",
       "1  I emailed chem prof sick said I either take pr...   \n",
       "2                                                NaN   \n",
       "3  I summarized summary bits affect students I wo...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            no_stops  \n",
       "0  I got cornell alert saying avoid arts quad any...  \n",
       "1  I emailed chem prof sick said I either take pr...  \n",
       "2                                                NaN  \n",
       "3  I summarized summary bits affect students I wo...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts = labeled.new_body.values\n",
    "X = vectorizer.fit_transform(posts)\n",
    "\n",
    "labels = labeled.Label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, val_input, train_label, val_label = train_test_split(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 6355), (240,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape, train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.82866043613707\n"
     ]
    }
   ],
   "source": [
    "baseline = 100 - (len(labeled.loc[labeled.Label == 1])/ len(labeled.new_body))\n",
    "\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(train_input, train_label)\n",
    "print(model.score(val_input, val_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed attempt at trying to implement a Bert model to detect satire in my reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pedro\\anaconda3\\envs\\3350\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text I just got a cornell alert saying to avoid the arts quad  anyone have any idea whats happening   Everyone please be safe and hopefully this is resolved soon   Edit  We are going to just get ahead of this and turn this into a megathread for this situation   Edit    We put the subreddit into restricted mode temporarily to avoid the mountain of posts  New posts will not show up for now  but you can still comment  Edit    If you see people giving advice such as leaving where they are sheltered  please report it  we dont want people to get hurt for following bad advice  Everyone please stay safe and locked down unless directed to do otherwise by authorities   Edit    Cornell confirmed that there is a bomb threat  We still dont know if there is anything else  so please stay safe and locked down   Edit    We took the subreddit out of restricted mode  Still keep all stuff related to this in this thread  but you can post again  \n"
     ]
    }
   ],
   "source": [
    "print('Actual text', posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['i', 'just', 'got', 'a', 'cornell', 'alert', 'saying', 'to', 'avoid', 'the', 'arts', 'quad', 'anyone', 'have', 'any', 'idea', 'what', '##s', 'happening', 'everyone', 'please', 'be', 'safe', 'and', 'hopefully', 'this', 'is', 'resolved', 'soon', 'edit', 'we', 'are', 'going', 'to', 'just', 'get', 'ahead', 'of', 'this', 'and', 'turn', 'this', 'into', 'a', 'mega', '##th', '##rea', '##d', 'for', 'this', 'situation', 'edit', 'we', 'put', 'the', 'sub', '##red', '##dit', 'into', 'restricted', 'mode', 'temporarily', 'to', 'avoid', 'the', 'mountain', 'of', 'posts', 'new', 'posts', 'will', 'not', 'show', 'up', 'for', 'now', 'but', 'you', 'can', 'still', 'comment', 'edit', 'if', 'you', 'see', 'people', 'giving', 'advice', 'such', 'as', 'leaving', 'where', 'they', 'are', 'sheltered', 'please', 'report', 'it', 'we', 'don', '##t', 'want', 'people', 'to', 'get', 'hurt', 'for', 'following', 'bad', 'advice', 'everyone', 'please', 'stay', 'safe', 'and', 'locked', 'down', 'unless', 'directed', 'to', 'do', 'otherwise', 'by', 'authorities', 'edit', 'cornell', 'confirmed', 'that', 'there', 'is', 'a', 'bomb', 'threat', 'we', 'still', 'don', '##t', 'know', 'if', 'there', 'is', 'anything', 'else', 'so', 'please', 'stay', 'safe', 'and', 'locked', 'down', 'edit', 'we', 'took', 'the', 'sub', '##red', '##dit', 'out', 'of', 'restricted', 'mode', 'still', 'keep', 'all', 'stuff', 'related', 'to', 'this', 'in', 'this', 'thread', 'but', 'you', 'can', 'post', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens\", tokenizer.tokenize(posts[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ids [1045, 2074, 2288, 1037, 10921, 9499, 3038, 2000, 4468, 1996, 2840, 17718, 3087, 2031, 2151, 2801, 2054, 2015, 6230, 3071, 3531, 2022, 3647, 1998, 11504, 2023, 2003, 10395, 2574, 10086, 2057, 2024, 2183, 2000, 2074, 2131, 3805, 1997, 2023, 1998, 2735, 2023, 2046, 1037, 13164, 2705, 16416, 2094, 2005, 2023, 3663, 10086, 2057, 2404, 1996, 4942, 5596, 23194, 2046, 7775, 5549, 8184, 2000, 4468, 1996, 3137, 1997, 8466, 2047, 8466, 2097, 2025, 2265, 2039, 2005, 2085, 2021, 2017, 2064, 2145, 7615, 10086, 2065, 2017, 2156, 2111, 3228, 6040, 2107, 2004, 2975, 2073, 2027, 2024, 18304, 3531, 3189, 2009, 2057, 2123, 2102, 2215, 2111, 2000, 2131, 3480, 2005, 2206, 2919, 6040, 3071, 3531, 2994, 3647, 1998, 5299, 2091, 4983, 2856, 2000, 2079, 4728, 2011, 4614, 10086, 10921, 4484, 2008, 2045, 2003, 1037, 5968, 5081, 2057, 2145, 2123, 2102, 2113, 2065, 2045, 2003, 2505, 2842, 2061, 3531, 2994, 3647, 1998, 5299, 2091, 10086, 2057, 2165, 1996, 4942, 5596, 23194, 2041, 1997, 7775, 5549, 2145, 2562, 2035, 4933, 3141, 2000, 2023, 1999, 2023, 11689, 2021, 2017, 2064, 2695, 2153]\n"
     ]
    }
   ],
   "source": [
    "print('Token to ids', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(posts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for post in posts:\n",
    "    max_len = max(max_len, len(str(post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_input_for_bert(posts, max_len):\n",
    "    #tokenize and map senteces to word ID\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    i = 0\n",
    "    for post in posts:\n",
    "        if(i<3):\n",
    "            print(\"Post\", posts)\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            post,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True\n",
    "            )\n",
    "        if (i<3):\n",
    "            print(\"dict\", encoded_dict['input_ids'])\n",
    "        input_ids.append(encoded_dict[\"attention_mask\"])\n",
    "        \n",
    "        i = (i+1)\n",
    "    \n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    return(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_inp, train_mask = mask_input_for_bert(train_input, max_len)\n",
    "# val_inp, val_mask = mask_input_for_bert(val_input, max_len)\n",
    "# train_label = tf.convert_to_tensor(train_label)\n",
    "# val_label = tf.convert_to_tensor(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_input_shape (240, 9336)\n",
      "Train_mask_shape (0,)\n",
      "Validation_input_shape (81, 9336)\n",
      "Validation_mask_shape (0,)\n",
      "Train_labelshape (240,)\n",
      "Validation label shape (81,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train_input_shape\", train_inp.shape)\n",
    "print(\"Train_mask_shape\", train_mask.shape)\n",
    "print(\"Validation_input_shape\", val_inp.shape)\n",
    "print(\"Validation_mask_shape\", val_mask.shape)\n",
    "print(\"Train_labelshape\", train_label.shape)\n",
    "print(\"Validation label shape\", val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Discussion and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The purpose of this experiment is to see if satire can be detected on the cornell subreddit. My motivation for this porject was due to the recent surge in non-sincere content on the cornell reddit. Due to the nature of reddit. It being a place where a user can remain totally anonymous I believed that the amount of disingenuous post would be relatively high. \n",
    "\n",
    "In conducting this project I had to overcome many hurdle. The first of these hurdles was learning how to scrape data from reddit. The reddit API was a fantastic start for this kind of task. I also recieved a tremendous amount of help from this medium article: https://medium.com/swlh/scraping-reddit-using-python-57e61e322486. \n",
    "After the post data was scrapped from the reddit I realized that the posts needed to be cleaned. Since many of them contained non-ascii characters that would not be useful to a text analysis project. \n",
    "\n",
    "My second large hurdle for this porject was the absence of gold labels for my data. In preparring for this project I was able to find many useful examples online of people scrapping twitter data to conduct sentiment analysis. A very useful article is linked here: https://thecleverprogrammer.com/2021/08/24/sarcasm-detection-with-machine-learning/\n",
    "\n",
    "I was able to find a data set of news article headlines and from this data set I was able to train a Naive Bayes model to detect satire. This model was trianed on the new article data and then implimented on the reddit data that I gathered. The naive bayed classifier was 84% accurate on the news article data. When a similiar model was trained and and evaluated on the reddit data its accuracy fell to 77%. Although an accuracy of 77% is less than the news article data I believe that it was a good first step in trying to detect sarcams using a model. Unfortunely the reddit data that I scraped also had to be hand labled meaning that I had to read through a dataset of 1000 reddit post and manually determine if the post were genuine or that. Although my initial assumption was that a large number of reddit posts were satirical, it turns out that only about 1% were in fact satire. This obviously had a huge effect on my project, since it entailed that I had very little data to train and test the model.\n",
    "\n",
    "The model's ended up not beating the baseline, and becuase of this I thought I shoudl try training the model on a bert pre-trained model, however after a lot of debugging and reading the huggingface API I was unable to get the bert-model to work. \n",
    "\n",
    "Overall, I believe that this project was very ambitious. I learned how to scrape data from reddit. I learned the difficulties of trying to manually annotate data and I was able to deploy a machine learning model on a cleaned text dataset of my own creation. Much of my time was dedicated to learing thr Bert API and unfortunately because of this I was unable to detect more time to an indepth analysis of the data that was scraped. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
